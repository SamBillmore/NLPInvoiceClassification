{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This code was used to investigate the data and identify the best preprocessing steps and the best model for the data. Therefore the code isvery messy and exploratory in nature. The final code used when actually running the preprocessing and the model is cleansed code based on the knowledge gained from this exploration.\n",
    "\n",
    "I have included comments in this file to explain what I did and hopefully give insight into the process of investigation that I followed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**\n",
    "\n",
    "The purpose of this project is to classify invoices into one of approximately 20 different accounting classifications based on information from the face of the invoice, such as the invoice description, the supplier company name, the amount of invoice and the currency.\n",
    "\n",
    "This is a real world problem encountered by a number of companies. This project is a proof of concept for a wider, more in depth project to potentially develop a more general model and, assuming it provides sufficiently good results, implement it into production environments.\n",
    "\n",
    "**Metric(s)**\n",
    "\n",
    "As this is a classification problem with the focus being on accurate classification of invoices into the relevant categories (with limited negative effects for incorrect classification) the key metrics for this project are: \n",
    "- the accuracy score and \n",
    "- the area under the receiver operating characteristic curve (AUC-ROC).\n",
    "\n",
    "**Data**\n",
    "\n",
    "The data has been sourced from a company's accounting systems and is real data that, unfortunately, I cannot share. A data dictionary has been provided below.\n",
    "\n",
    "**Findings**\n",
    "\n",
    "It is possible to predict the correct invoice classification using data from the face of the invoice with an accuracy of approximately **94.4%** achieved on the test data set.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "The data available has 2.18 million rows and, due to data sharing restrictions, all processing and analysis had to be completed on my personal laptop. In order to work with the data in a reasonable time frame with the limited computing power, I worked with a subset of the data, created by selecting those invoices in GBP with no null values in the required columns.\n",
    "\n",
    "Therefore the resulting model is specific to those invoices in GBP (which are likely to have an invoice description in English) which have data in all required columns.\n",
    "\n",
    "Additional work will need to be completed to generalise the model to non-GBP invoices and also to be able to handle null values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model walk through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers the entire scope of the project, from data collection through exploratory data analysis (EDA), data cleansing and processing through to modelling and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been collected from a number of different accounting systems owned by the global group of companies. This data is being used for a number of different purposes so there has been an amount of pre-processing and cleansing that had already happened to the data before I have access to it. This included:\n",
    "\n",
    "- Combining multiple import tables from different systems into a single table\n",
    "- Standardising the different columns (including the classification groups - the target)) across the different source tables\n",
    "- Adding the local currency conversions\n",
    "- Deleting records where certain criteria is met\n",
    "    \n",
    "To access the data from the Microsoft SQL Server database I used the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import credentials # A file containing the required credentials for accessing the database\n",
    "\n",
    "# Set jupyter notebook settings to be able to view up to 999 columns of a pandas dataframe without concatenation\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "# Set up variables for database access\n",
    "db_driver = credentials.database_driver\n",
    "db_server = credentials.database_server\n",
    "db_database = credentials.database\n",
    "sql_schema = credentials.schema\n",
    "sql_table = credentials.table\n",
    "\n",
    "# Set up the connection to the database\n",
    "db_creds = 'DRIVER={'+db_driver+'}; SERVER='+db_server+'; DATABASE='+db_database+';Trusted_Connection=yes;'\n",
    "cnxn = pyodbc.connect(db_creds)\n",
    "\n",
    "# Create query for selecting columns from required table, filtering for non-intercompany rows\n",
    "sql_columns_filtered = \"[Invoice Operating Unit],[Business Unit],[Service Line],[Region],[OU/Country]\\\n",
    ",[Supplier Number],[Supplier Name],[Invoice Type],[Invoice Date],[Paid Date],[Invoice Desc],[PO Number]\\\n",
    ",[Invoice_Amt],[Invoice Currency],[USD_Amt],[V Site Country],[Pay Group],[Payment Status],[Project Owning Org]\\\n",
    ",[Project Number],[Country],[datafile],[datasource],[Legacy],[Year],[Contract_type],[Customer],[Service_line]\\\n",
    ",[Funding_structure],[Category_Group],[Supplier_Group],[Supplier_Sector],[Leakage_Identifier],[Leakage_Group]\\\n",
    ",[Serv_Mtrl],[Reimbursable_Flag],[Intercompany_Flag],[Americas_Flag],[Reporting_Date]\"\n",
    "\n",
    "sql_query_filtered = 'select '+sql_columns_filtered+' from '+sql_schema+'.'+sql_table\\\n",
    "+\" where [Intercompany_Flag] = 'Non-intercompany'\"\n",
    "\n",
    "# Read the data from the database into a pandas dataframe\n",
    "data_filtered = pd.read_sql(sql_query_filtered,cnxn)\n",
    "\n",
    "# Save the data to a local .csv file\n",
    "data_filtered.to_csv('data_2018_12_07.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved the data to a csv file locally so I had a fixed baseline to work with that wouldn't change if the database were updated. A data dictionary is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by looking at the columns available in the data and understanding what type of information the columns were supposed to contain. I created a data dictionary to record this information (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start my exploratory data analysis (EDA) I looked at the data in first few rows to see if the data matched the understanding based on the data dictionary. In addition I set the data types for each column and set to null those entries that didn't match the expected data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set all data types to string for initial import\n",
    "col_dtypes = {'Invoice Operating Unit':str, 'Business Unit':str, 'Service Line':str, 'Region':str,\n",
    "       'OU/Country':str, 'Supplier Number':str, 'Supplier Name':str, 'Invoice Type':str,\n",
    "       'Invoice Date':str, 'Paid Date':str, 'Invoice Desc':str, 'PO Number':str, 'Invoice_Amt':str,\n",
    "       'Invoice Currency':str, 'USD_Amt':float, 'V Site Country':str, 'Pay Group':str,\n",
    "       'Payment Status':str, 'Project Owning Org':str, 'Project Number':str, 'Country':str,\n",
    "       'datafile':str, 'datasource':str, 'Legacy':str, 'Year':str, 'Contract_type':str, 'Customer':str,\n",
    "       'Service_line':str, 'Funding_structure':str, 'Category_Group':str, 'Supplier_Group':str,\n",
    "       'Supplier_Sector':str, 'Leakage_Identifier':str, 'Leakage_Group':str, 'Serv_Mtrl':str,\n",
    "       'Reimbursable_Flag':str, 'Intercompany_Flag':str, 'Americas_Flag':str,\n",
    "       'Reporting_Date':str}\n",
    "\n",
    "# Import the data from the .csv file\n",
    "data = pd.read_csv('data_2018_12_07.csv',na_values='Unknown',dtype=col_dtypes)\n",
    "\n",
    "#Check the head of the data\n",
    "data.head()\n",
    "\n",
    "# Set the data types of the relevant columns to the correct data type\n",
    "data['Invoice Date'] = pd.to_datetime(data['Invoice Date'],errors='coerce')\n",
    "data['Paid Date'] = pd.to_datetime(data['Paid Date'],errors='coerce')\n",
    "data['Reporting_Date'] = pd.to_datetime(data['Reporting_Date'],errors='coerce')\n",
    "data['Supplier Number'] = pd.to_numeric(data['Supplier Number'],errors='coerce')\n",
    "data['Invoice_Amt'] = pd.to_numeric(data['Invoice_Amt'],errors='coerce')\n",
    "data['USD_Amt'] = pd.to_numeric(data['USD_Amt'],errors='coerce')\n",
    "data['Year'] = pd.to_numeric(data['Year'],errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then looked at summary information such as the total number of records, the number of nulls by column and the number of unique elements per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data\n",
    "data.shape\n",
    "# Output: (2189333, 39)\n",
    "\n",
    "# Percentage of each column that is null\n",
    "data.isnull().sum()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next step was to visualise the data in Tableau to understand a bit more what the data contained and quickly interrogate and investigate what was available. One of the views I created was the ability to select a column and view the different elements available in that column, the count of those elements and whether they were nulls, unclassified, blank, zero or otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau workbook not available due to data sharing restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most important columns are the target (invoice classification) and invoice description (as it is likely to be a strong predictor of the target). I reviewed these two columns in more detail including: reviewing the class imbalance and identifying the number of unclassified invoices for the target; and looking at a random sample of the invoice descriptions to understand what data they contain and whether it was as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand class imbalance\n",
    "data['Category_Group'].value_counts()\n",
    "\n",
    "# Output:\n",
    "\n",
    "# Category 1     637855\n",
    "# Category 2     439340    NB: This is the 'Unclassified' category\n",
    "# Category 3     206532\n",
    "# Category 4     185722\n",
    "# Category 5     106766\n",
    "# Category 6     96262\n",
    "# Category 7     69142\n",
    "# Category 8     64971\n",
    "# Category 9     63096\n",
    "# Category 10    60642\n",
    "# Category 11    59679\n",
    "# Category 12    45523\n",
    "# Category 13    39289\n",
    "# Category 14    31918\n",
    "# Category 15    20553\n",
    "# Category 16    20341\n",
    "# Category 17    14289\n",
    "# Category 18    7945\n",
    "# Category 19    7242\n",
    "# Category 20    639\n",
    "# Category 21    626\n",
    "# Category 22    268\n",
    "# Category 23    114\n",
    "# Category 24    51\n",
    "# Category 25    50\n",
    "# Category 26    41\n",
    "# Category 27    16\n",
    "# Category 28    15\n",
    "# Category 29    13\n",
    "# Category 30    12\n",
    "# Category 31    11\n",
    "# Category 32    7\n",
    "# Category 33    5\n",
    "# Category 34    4\n",
    "# Category 35    3\n",
    "# Category 36    3\n",
    "\n",
    "# Create random sample for review\n",
    "# NB: there is no no random state defined so each time it is run it provides a different sample\n",
    "data.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete data set contains over 2.18 million rows of data from a wide variety of source systems, countries and languages. As such there are a large number of nulls, multiple languages, multiple character sets in text data, string values in numerical columns and category groups like 'Unclassified', 'Intercompany' and 'Do Not Use'.\n",
    "\n",
    "Due to the restrictions on where I could host the commercially sensitive data, I would have to work purely from my laptop (rather than using cloud based services like AWS). Therefore, in order to be able to process the data in a reasonable time period I had to select a subset of the data to work with.\n",
    "\n",
    "(It took a lot of trial and error to work out the size of the dataset that would work best - giving the ability to work on my laptop whilst still retaining enough data to make the model a true proof of concept for further development for the wider data set.)\n",
    "\n",
    "When creating this subset I considered:\n",
    "- future natural language processing would be significantly easier if the data were all in the same language\n",
    "- as category group is the target variable, it made sense to exclude those category groups that are unwanted\n",
    "- some columns had very high levels of null values, so it made sense to exclude them\n",
    "- there is very high class imbalance, so reducing the size of the data at this stage would help keep the training data set small (even after any oversampling to redress the imbalance)\n",
    "\n",
    "Therefore I:\n",
    "- dropped columns with high levels of null values, \n",
    "- dropped rows with unwanted category groups (the target)\n",
    "- dropped remaining rows with any null values, and \n",
    "- only selected those invoices with a local currency of GBP.\n",
    "\n",
    "This last point was on the assumption that an invoice in GBP was likely to have an English description.\n",
    "\n",
    "**NB:** I did attempt to identify the invoice description language row by row using the Python library 'langdetect'. However, due to the high level of technical language and the low numbers of words in the invoice descriptions, this approach did not return any sensible results. For example, 'WATER 10L' was identified as German, even though the invoice was in GBP and the supplier was a British company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Drop rows for 'UNCLASSIFIED','INTERCOMPANY','DONOTUSE-SUBCONTRACTOR'\n",
    "data_processing = data[~data['Category_Group'].isin(['UNCLASSIFIED','INTERCOMPANY','DONOTUSE-SUBCONTRACTOR'])]\n",
    "\n",
    "# Keep columns with low percentage of null values\n",
    "column_mask = ['Business Unit', 'Supplier Number', 'Supplier Name', \n",
    "       'Invoice Date', 'Invoice Desc', 'Invoice_Amt',\n",
    "       'Invoice Currency', 'USD_Amt', \n",
    "       'Project Owning Org', \n",
    "       'datasource', 'Legacy', 'Year', \n",
    "       'Category_Group', 'Supplier_Group',\n",
    "       'Leakage_Identifier', 'Leakage_Group', 'Intercompany_Flag', 'Americas_Flag',\n",
    "       'Reporting_Date']\n",
    "\n",
    "data_processing = data_processing[column_mask].copy()\n",
    "\n",
    "# Drop remaining nulls\n",
    "data_processing.dropna(inplace=True)\n",
    "\n",
    "# Retain only invoices in GBP\n",
    "data_processing_english = data_processing[data_processing['Invoice Currency'].isin(['GBP'])]\n",
    "\n",
    "# Remaining records\n",
    "data_processing_english.shape\n",
    "# Output:\n",
    "# (391410, 19)\n",
    "\n",
    "# Save to .csv\n",
    "data_processing_english.to_csv('Cleansed data_GBP.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempted language detection** (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from datetime import datetime\n",
    "\n",
    "# Sub set the data for testing purposes\n",
    "data_processing_1 = data_processing_english.iloc[0:10000].copy()\n",
    "\n",
    "# Set a seed so the results are consistent each time it is run\n",
    "# The algorithm is probabilistic based and, for low word counts, different results are obtained for different seeds\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Initiate a timer\n",
    "startTime = datetime.now()\n",
    "\n",
    "output_1 = []\n",
    "count = 0\n",
    "for text in data_processing_1['Invoice Desc']:\n",
    "    # Try except statement captures those instances where the algorithm is unable to classify the language\n",
    "    try:\n",
    "        out = detect(text)\n",
    "    except:\n",
    "        out = 'n/a'\n",
    "    output_1.append(out)\n",
    "    # A counter to show progress\n",
    "    count+=1\n",
    "    if count%1000 == 0:\n",
    "        print(count)\n",
    "        \n",
    "# Print time taken to run\n",
    "print(datetime.now() - startTime)\n",
    "\n",
    "# Add the output to an additional column in the dataframe for review purposes\n",
    "data_processing_1['language'] = output_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data processing stage is where ther majority of the time and effort was spent, attempting to identify which elements of the data provided the best predictive ability.\n",
    "\n",
    "The first step was to split the data into a train and test set, with the test data then able to be used later to confirm how well the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('Cleansed data_GBP.csv',na_values='Unknown')\n",
    "\n",
    "X = data.copy()\n",
    "y = X.pop('Category_Group')\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing steps are different for the train and the test sets, as the training data is used to fit the data conversion steps whereas the test data is purely converted and then used in the model to generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step for the training data is to start to rebalance the classes. This step is split into three elements:\n",
    "\n",
    "1. remove those classes with very few counts\n",
    "2. randomly undersample those classes with very high counts\n",
    "3. use a k-nearest-neighbours approach to generate more synthetic counts for those classes with low counts\n",
    "\n",
    "As step three uses a KNN model to generate the synthetic points, the data needs to be in a format suitable for modelling. Therefore, this is done as the final preprocessing step prior to modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rebalance classes - Step 1 - removing classes with very few counts**\n",
    "\n",
    "This step simply removes those classes which have very low counts. This means these classes will never be predicted by the model but as they are so small, the trade off should be minimal in terms of overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for each category\n",
    "data_value_counts = pd.DataFrame(y_train.value_counts(dropna=False))\n",
    "\n",
    "# Create a list of those categories with counts > 100\n",
    "categories_not_low = data_value_counts[data_value_counts['Category_Group']>100].index\n",
    "\n",
    "# Subset y_train for those categories that are not low\n",
    "y_train = y_train[y_train.isin(categories_not_low)]\n",
    "\n",
    "# Subset X_train for the indexes in y_train\n",
    "X_train = X_train.loc[y_train.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rebalance classes - Step 2 - random undersampling**\n",
    "\n",
    "This step splits the data into two sections: the first with those classes with high counts per category; and the second the remainder.\n",
    "\n",
    "The first set is the randomly undersampled to generate a data set with equal class counts.\n",
    "\n",
    "This undersampled first set is then combined with the second 'remainder' set to recreate the X_train and y_train sets but with those large categories undersampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Create a list of those categories with counts > 10,000\n",
    "categories_too_high = data_value_counts[data_value_counts['Category_Group']>10000].index\n",
    "\n",
    "# Create a subset of y_train and X_train filtered for those high count categories\n",
    "y_train_too_high = y_train[y_train.isin(categories_too_high)].copy()\n",
    "X_train_too_high = X_train.loc[y_train_too_high.index].copy()\n",
    "\n",
    "# Create a subset of y_train and X_train excluding those high count categories\n",
    "y_train_remainder = y_train[~y_train.isin(categories_too_high)].copy()\n",
    "X_train_remainder = X_train.loc[y_train_remainder.index].copy()\n",
    "\n",
    "# Instantiate the Random Under Sampler\n",
    "under_sampler = RandomUnderSampler()\n",
    "\n",
    "# Under sample the high count category subsets\n",
    "X_train_undersampled, y_train_undersampled = under_sampler.fit_sample(X_train_too_high,y_train_too_high)\n",
    "\n",
    "# Convert the output back to a pandas format and append to the remainder sets\n",
    "y_train_undersampled = pd.Series(y_train_undersampled)\n",
    "y_train = y_train_undersampled.append(y_train_remainder,ignore_index=True)\n",
    "\n",
    "X_train_undersampled = pd.DataFrame(X_train_undersampled,columns=X_train.columns)\n",
    "X_train = X_train_undersampled.append(X_train_remainder,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning text**\n",
    "\n",
    "After viewing a random sample of the text data, there were some specific items that needed to be resolved, as well as applying some standard natural language processing steps.\n",
    "\n",
    "The steps are:\n",
    "- remove specific characters (\\r, \\n)\n",
    "- expand linguistic contractions (e.g. don't => do not)\n",
    "- tokenise the text strings\n",
    "- remove non-ascii characters\n",
    "- convert all text to lowercase\n",
    "- remove punctuation\n",
    "- obtain the stem for each word\n",
    "\n",
    "Each step contains a trade off in understanding of text. For example, converting text to lowercase removes information about words that start a sentence, or are a name rather than another word (e.g. Apple the company and apple the fruit). However, after running a basic decision tree model on the output for each combination of processing steps, this combination of processing steps seemed to provide the best.\n",
    "\n",
    "In addition, I tried replacing numerical values with their string equivalent (e.g. 2 becomes two) but this did not seem to improve the quality of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions, unicodedata, re\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "X_train = X_train.copy()\n",
    "\n",
    "# Replace specific characters\n",
    "replace_dict = {\n",
    "    '\\r':' ',\n",
    "    '\\n':' '\n",
    "}\n",
    "\n",
    "for key,value in replace_dict.items():\n",
    "    X_train['Invoice Desc'] = [text.replace(key,value) for text in X_train['Invoice Desc']]\n",
    "    \n",
    "# Fix contractions (e.g. can't => cannot)\n",
    "X_train['Invoice Desc'] = [contractions.fix(text) for text in X_train['Invoice Desc']]\n",
    "\n",
    "# Tokenise\n",
    "X_train['Invoice Desc'] = [nltk.word_tokenize(text) for text in X_train['Invoice Desc']]\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "# Run functions above to remove non-ascii characters, \n",
    "# convert to lowercase, remove punctuation and obtain the word stems\n",
    "t0 = time.time()\n",
    "X_train['Invoice Desc'] = [remove_non_ascii(text) for text in X_train['Invoice Desc']]\n",
    "t1 = time.time()\n",
    "print('Non ascii: ',t1-t0)\n",
    "X_train['Invoice Desc'] = [to_lowercase(text) for text in X_train['Invoice Desc']]\n",
    "t2 = time.time()\n",
    "print('Lowercase: ',t2-t1)\n",
    "X_train['Invoice Desc'] = [remove_punctuation(text) for text in X_train['Invoice Desc']]\n",
    "t3 = time.time() \n",
    "print('Remove punctuation: ',t3-t2)\n",
    "t6 = time.time()\n",
    "X_train['Invoice Desc'] = [stem_words(text) for text in X_train['Invoice Desc']]\n",
    "t7 = time.time()\n",
    "print('Stem words: ',t7-t6)\n",
    "\n",
    "# Reconstruct description after tokenisation for use in CountVectorizer\n",
    "X_train['Invoice Desc'] = [' '.join(text) for text in X_train['Invoice Desc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function for replacing numbers with their string equivalent, which wasn't used in the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all integer occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "# Run replace_numbers function (not used)\n",
    "t4 = time.time()\n",
    "X_train['Invoice Desc'] = [replace_numbers(text) for text in X_train['Invoice Desc']]\n",
    "t5 = time.time()\n",
    "print('Stem words: ',t5-t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorising text**\n",
    "\n",
    "The next step was to vectorise the cleansed text using Count Vectorizer, as well as other columns including the supplier name, supplier group and internal project name. \n",
    "\n",
    "The choice to vectorise the supplier and project information was based on the theory that the names may reflect the type of industry or project and hence the potential classification that the invoices linked with them.\n",
    "\n",
    "I also tried simply dummifying the supplier and project columns (rather tha vectorising them) but the results were less good.\n",
    "\n",
    "The CountVectorizer returns a sparse matrix and, due to the size of the data and memory limitations from the laptop, I chose to keep the output in this format and also convert all the other data to sparse matrices and combine them, instead of trying to work with pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvec_supplier = CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    "cvec_invoice = CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    "cvec_currency = CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    "cvec_project = CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    "cvec_group = CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    "\n",
    "supplier_name_nlp = X_train['Supplier Name']\n",
    "invoice_desc_nlp = X_train['Invoice Desc']\n",
    "currency_nlp = X_train['Invoice Currency']\n",
    "project_nlp = X_train['Project Owning Org']\n",
    "group_nlp = X_train['Supplier_Group']\n",
    "\n",
    "cvec_supplier.fit(supplier_name_nlp)\n",
    "cvec_invoice.fit(invoice_desc_nlp)\n",
    "cvec_currency.fit(currency_nlp)\n",
    "cvec_project.fit(project_nlp)\n",
    "cvec_group.fit(group_nlp)\n",
    "\n",
    "supplier_sparse_matrix = cvec_supplier.transform(supplier_name_nlp)\n",
    "invoice_sparse_matrix = cvec_invoice.transform(invoice_desc_nlp)\n",
    "currency_sparse_matrix = cvec_currency.transform(currency_nlp)\n",
    "project_sparse_matrix = cvec_project.transform(project_nlp)\n",
    "group_sparse_matrix = cvec_group.transform(group_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummifying columns**\n",
    "\n",
    "The remaining columns containing data in string format were dummified using one-hot-encoding in pandas, then converted to a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dumm_cols = ['Business Unit','datasource','Legacy','Leakage_Identifier','Leakage_Group','Americas_Flag']\n",
    "\n",
    "dummified_data = pd.get_dummies(X_train[dumm_cols],drop_first=True)\n",
    "dummified_sparse_matrix = sparse.csr_matrix(dummified_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remaining columns**\n",
    "\n",
    "The remaining columns were converted to numerical values, then converted to a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_cols = ['Invoice_Amt','USD_Amt','Year']\n",
    "retain_data = pd.DataFrame()\n",
    "\n",
    "for col in retain_cols:\n",
    "    retain_data[col] = pd.to_numeric(X_train[col])\n",
    "    \n",
    "retain_sparse_matrix = sparse.csr_matrix(retain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine sparse matrices**\n",
    "\n",
    "The next step for the training data was to combine the individual sparse matrices into a single matrix, ready for using in the oversampling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrices = [\n",
    "    supplier_sparse_matrix,\n",
    "    invoice_sparse_matrix,\n",
    "    currency_sparse_matrix,\n",
    "    project_sparse_matrix,\n",
    "    group_sparse_matrix,\n",
    "    dummified_sparse_matrix,\n",
    "    retain_sparse_matrix\n",
    "]\n",
    "\n",
    "X_train_sparse = sparse.hstack(sparse_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature names**\n",
    "\n",
    "A list of the feature names is generated (for use during the model evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cvec_supplier.get_feature_names()\\\n",
    "                +cvec_invoice.get_feature_names()\\\n",
    "                +cvec_currency.get_feature_names()\\\n",
    "                +cvec_project.get_feature_names()\\\n",
    "                +cvec_group.get_feature_names()\\\n",
    "                +list(dummified_data.columns)\\\n",
    "                +list(retain_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rebalance classes - Step 3 - SMOTE oversampling**\n",
    "\n",
    "The final step for the training data was to use the SMOTE oversampling method to increase the count of those classes with few entries by creating synthetic values based on a K-Nearest-Neighbours approach. After this step then all the classes would be balanced and the data would be ready for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sampler = SMOTE(random_state=1,n_jobs=-1)\n",
    "X_resampled, y_resampled = sampler.fit_sample(X_train_sparse, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save training data to .csv**\n",
    "\n",
    "The final stage for the training data was to save it to .csv for use in the modelling stage. This meant the processing and oversampling did not need to be run each time a new model was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('X train_processed data_GBP.npz',X_resampled)\n",
    "pd.DataFrame(y_resampled).to_csv('y train_processed data_GBP.csv')\n",
    "pd.Series(feature_names).to_csv('X train_processed data_GBP_feature names.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data and training data without balancing the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a test set against which the trained model could be tested, a similar set of processing steps has to be applied to the training data. In addition, applying the same steps to the training data but without applying the rebalancing steps allows a comparison between training scores and test scores, and helps to determine how well the model generalises.\n",
    "\n",
    "**NB:** it is not possible to compare the rebalanced training scores against the test scores as the baselines are significantly different.\n",
    "\n",
    "The same process is applied to both the test data and the imbalanced training data. The code below is for the test data but it is identical for both.\n",
    "\n",
    "The code replicates the steps applied for the training data but does not rebalance the classes and only transforms the data (rather than refitting the vectoriser). In addition, the dummification of columns required ensuring the resulting columns matched with the dummified balanced training data (see code below).\n",
    "\n",
    "The data is retained as a sparse matrix as for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.copy()\n",
    "\n",
    "# Replace specific characters\n",
    "for key,value in replace_dict.items():\n",
    "    X_test['Invoice Desc'] = [text.replace(key,value) for text in X_test['Invoice Desc']]\n",
    "    \n",
    "# Fix contractions (e.g. can't => cannot)\n",
    "X_test['Invoice Desc'] = [contractions.fix(text) for text in X_test['Invoice Desc']]\n",
    "\n",
    "# Tokenise\n",
    "X_test['Invoice Desc'] = [nltk.word_tokenize(text) for text in X_test['Invoice Desc']]\n",
    "\n",
    "# Remove non-ascii characters, convert to lowercase and remove punctuation\n",
    "t0 = time.time()\n",
    "X_test['Invoice Desc'] = [remove_non_ascii(text) for text in X_test['Invoice Desc']]\n",
    "t1 = time.time()\n",
    "print('Non ascii: ',t1-t0)\n",
    "X_test['Invoice Desc'] = [to_lowercase(text) for text in X_test['Invoice Desc']]\n",
    "t2 = time.time()\n",
    "print('Lowercase: ',t2-t1)\n",
    "X_test['Invoice Desc'] = [remove_punctuation(text) for text in X_test['Invoice Desc']]\n",
    "t3 = time.time()\n",
    "print('Remove punctuation: ',t3-t2)\n",
    "\n",
    "# Stem the words\n",
    "t6 = time.time()\n",
    "X_test['Invoice Desc'] = [stem_words(text) for text in X_test['Invoice Desc']]\n",
    "t7 = time.time()\n",
    "print('Stem words: ',t7-t6)\n",
    "\n",
    "# Reconstruct description after tokenisation\n",
    "X_test['Invoice Desc'] = [' '.join(text) for text in X_test['Invoice Desc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step applies the existing fitted CountVectorizer to transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise using fitted CountVectorizer from training data\n",
    "supplier_name_nlp = X_test['Supplier Name']\n",
    "invoice_desc_nlp = X_test['Invoice Desc']\n",
    "currency_nlp = X_test['Invoice Currency']\n",
    "project_nlp = X_test['Project Owning Org']\n",
    "group_nlp = X_test['Supplier_Group']\n",
    "\n",
    "supplier_sparse_matrix = cvec_supplier.transform(supplier_name_nlp)\n",
    "invoice_sparse_matrix = cvec_invoice.transform(invoice_desc_nlp)\n",
    "currency_sparse_matrix = cvec_currency.transform(currency_nlp)\n",
    "project_sparse_matrix = cvec_project.transform(project_nlp)\n",
    "group_sparse_matrix = cvec_group.transform(group_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step dummifies the test data, then compares the columns generated by dummification between the test and the training data. Any columns that match are kept. Any columns in the test set but not in the train set are dropped and any columns in the train set but not in the test set are created and set to zero.\n",
    "\n",
    "The data is the converted to a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify test data\n",
    "dummified_data_test = pd.get_dummies(X_test[dumm_cols],drop_first=False)\n",
    "\n",
    "# Filter out dummified columns not in training data\n",
    "test_col_in_train = list(compress(dummified_data_test.columns, dummified_data_test.columns.isin(dummified_data.columns)))\n",
    "dummified_data_test = dummified_data_test[test_col_in_train]\n",
    "\n",
    "# Add columns in dummified train set that are not in dummified test set\n",
    "train_col_not_in_test = list(set(dummified_data.columns)-set(dummified_data_test.columns))\n",
    "for col in train_col_not_in_test:\n",
    "    dummified_data_test[col] = 0\n",
    "    \n",
    "# Convert to sparse matrix\n",
    "dummified_sparse_matrix = sparse.csr_matrix(dummified_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining (numerical) columns are converted to numeric format and then converted to a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric columns to numeric data type\n",
    "retain_data = pd.DataFrame()\n",
    "for col in retain_cols:\n",
    "    retain_data[col] = pd.to_numeric(X_test[col])\n",
    "    \n",
    "# Convert to sparse matrix\n",
    "retain_sparse_matrix = sparse.csr_matrix(retain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step combines the sparse matrices together to greate the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrices = [\n",
    "    supplier_sparse_matrix,\n",
    "    invoice_sparse_matrix,\n",
    "    currency_sparse_matrix,\n",
    "    project_sparse_matrix,\n",
    "    group_sparse_matrix,\n",
    "    dummified_sparse_matrix,\n",
    "    retain_sparse_matrix\n",
    "]\n",
    "\n",
    "X_test_sparse = sparse.hstack(sparse_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then saved to .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('X test_processed data_GBP.npz',X_test_sparse)\n",
    "pd.DataFrame(y_test).to_csv('y test_processed data_GBP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import data and calculate baselines**\n",
    "\n",
    "The first step is to import the data and to calculate the baseline values for each data set. The baseline shows the accurancy score that would be achieved purely by predicting the most common occuring value as the prediction for all inputs.\n",
    "\n",
    "The baseline for the balanced training data is significantly below the unbalanced training data, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Import balanced training, unbalanced training and test data\n",
    "X_train = sparse.load_npz('X train_processed data_GBP.npz')\n",
    "y_train = pd.read_csv('y train_processed data_GBP.csv')\n",
    "y_train.columns = ['index','y']\n",
    "y_train = y_train['y']\n",
    "X_test = sparse.load_npz('X test_processed data_GBP.npz')\n",
    "y_test = pd.read_csv('y test_processed data_GBP.csv')\n",
    "y_test.columns = ['index','y']\n",
    "y_test = y_test['y']\n",
    "X_train_imbalanced = sparse.load_npz('X test_processed data_GBP.npz')\n",
    "y_train_imbalanced = pd.read_csv('y test_processed data_GBP.csv')\n",
    "y_train_imbalanced.columns = ['index','y']\n",
    "y_train_imbalanced = y_train_imbalanced['y']\n",
    "\n",
    "\n",
    "# Calculate baselines\n",
    "baseline_train = (y_train.value_counts()/len(y_train))[0]\n",
    "baseline_train\n",
    "\n",
    "# Output: 0.05555555...\n",
    "\n",
    "baseline_train_imbalanced = (y_train_imbalanced.value_counts()/len(y_train_imbalanced))[0]\n",
    "baseline_train_imbalanced\n",
    "\n",
    "# Output: 0.48042206...\n",
    "\n",
    "baseline_test = (y_test.value_counts()/len(y_test))[0]\n",
    "baseline_test\n",
    "\n",
    "# Output: 0.48042206..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial models**\n",
    "\n",
    "The next stage is to try lots of models against the data and determine which model is worth investigating in more detail. This is determined by a couple of factors, including the accuracy score as well as the length of time the model takes to run.\n",
    "\n",
    "In order to test each model, a cross validation was applied, which uses generated splits in the training data to determine how well the model will generalise to unseen data. The model was then fitted on the entire balanced training data and assessed against both the unbalanced training data and the test data. In total this provides three scores:\n",
    "- cross validated score\n",
    "- unbalanced training score\n",
    "- test score\n",
    "\n",
    "The cross validated score is used to determine between models which is the best model and the unbalanced score can be compared to the test score to confirm that the model generalises well.\n",
    "\n",
    "Below is the code for the decision tree classifier. The code is identical for the other models, just with a different model defined (instead of dt_model = DecisionTreeClassifier()), so it is not reproduced here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the models and scores is provided:\n",
    "\n",
    "|Model<br>|Cross validated score<br>(baseline = 0.056)|Test data score<br>(baseline = 0.480)|Time taken for cross<br>validation (seconds)|\n",
    "|---|---|---|---|\n",
    "|Decision Tree|0.402|0.393|42.594|\n",
    "|Logistic Regression|0.581|0.624|548.918|\n",
    "|XG Boost|0.905|0.848|2824.799|\n",
    "|Random Forest|0.962|0.937|213.555|\n",
    "|Bagging (Decision tree)|0.959|0.936|1087.413|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision tree model**\n",
    "\n",
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "dt_model = DecisionTreeClassifier(criterion='gini', max_depth=10)\n",
    "dt_scores = cross_val_score(dt_model, X_train, y_train, cv=skf, n_jobs=-1,verbose=1)\n",
    "dt_scores\n",
    "t1 = time.time()\n",
    "print('Time taken: ',t1-t0)\n",
    "print('Baseline: ',baseline_train)\n",
    "print('Accuracy score: ',dt_scores.mean())\n",
    "\n",
    "# Output:\n",
    "# Time taken:  42.5940146446228\n",
    "# Baseline:  0.05555555555555555\n",
    "# Accuracy score:  0.4017963755481587"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(X_train,y_train)\n",
    "dt_test_predictions = dt_model.predict(X_test)\n",
    "print('Baseline test: ',baseline_test)\n",
    "print('Accuracy score: ',accuracy_score(y_test,dt_test_predictions))\n",
    "\n",
    "# Output:\n",
    "# Baseline test:  0.48042206382054625\n",
    "# Accuracy score:  0.39279527860810914"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further model refinement**\n",
    "\n",
    "The next step is to determine which models to review in further detail and attempt to refine. As the random forest had the highest score and had a reasonable run time, this was the model chosen for further refinement.\n",
    "\n",
    "There are limited parameters available for tuning a random forest. The main way to improve accuracy is by increasing the number of individual decision trees in the forest. However, the depth of each individual tree could be adjusted to determine if that increase the quality of prediction (although it is likely that a high depth will return the best result). In addition, the function used to measure the quality of a split can be adjusted (although this will likely have minimal impact).\n",
    "\n",
    "In order to test multiple combinations of parameters for a model, a gridsearch can be applied, which iterates through each combination of parameters defined and calculates the score for each. The best combination of parameters and the resulting score can then be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "rf_params = {\n",
    "    'criterion':['gini','entropy'],\n",
    "    'max_depth':[None,20,40,60]\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "\n",
    "rf_gridsearch = GridSearchCV(rf_model,\n",
    "                              rf_params,\n",
    "                              n_jobs=-1, cv=5, verbose=1)\n",
    "\n",
    "rf_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Time taken: ',t1-t0)\n",
    "print('Score: ',rf_gridsearch.best_score_)\n",
    "print('Best parameters: 'rf_gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model used was a Random Forest with 100 trees, each with no limit on the depth and using the gini criterion to measure the quality of each split in each tree. The outputs of the model (including the feature importances) were saved to .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "rf_model.fit(X_train,y_train)\n",
    "rf_train_imbalanced_predictions = rf_model.predict(X_train_imbalanced)\n",
    "print('Baseline - training imbalanced: ',baseline_train_imbalanced)\n",
    "print('Accuracy - training imbalanced: ',accuracy_score(y_train_imbalanced,rf_train_imbalanced_predictions))\n",
    "\n",
    "# Output:\n",
    "# Baseline - training imbalanced:  0.48042206382054625\n",
    "# Accuracy - training imbalanced:  0.9707978845711658\n",
    "\n",
    "rf_test_predictions = rf_model.predict(X_test)\n",
    "print('Baseline - test: ',baseline_test\n",
    "print('Accuracy - test: ',accuracy_score(y_test,rf_test_predictions))\n",
    "\n",
    "# Output:\n",
    "# Baseline - test:  0.48042206382054625\n",
    "# Accuracy - test:  0.9438747093840218\n",
    "    \n",
    "# Save outputs to .csv\n",
    "pd.Series(rf_test_predictions).to_csv('Random forest predictions.csv')\n",
    "y_test.to_csv('Actuals.csv')\n",
    "pd.Series(rf_model.feature_importances_).to_csv('Random forest feature importance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly some model evaluation has occured already in order to determine which model to investigate in more detail.\n",
    "\n",
    "However, model evaluation include more than reviewing the accuracy scores and the run time for the model. It also includes (for classification problems) reviewing the confusion matrix, the classification report, the ROC curve and calculating the area under the ROC curve (AUC-ROC).\n",
    "\n",
    "In addition, reviewing the feature importances (if available) can provide additional information into what features the model is using for predictions. Comparing the feature importance across models can also help determine if the features are reasonably stable and likely to be true predictors or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix and classification report**\n",
    "\n",
    "The confusion matrix shows where the predicted classifications match the actual classificationsand, where they don't, the class that has been incorrectly predicted. It can be used to determine whether there are particular classes that the model consistently classifies incorrectly and then additional features or other changes could be implemented to attempt to rectify this.\n",
    "\n",
    "In addition, there are other scores available via the classification report, including the precision, recall and f1 scores. In general, the closer to 1 that these values are then the better the model performs.\n",
    "\n",
    "The class names have been changed to ensure sentitive data is not shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import data\n",
    "predictions = pd.read_csv('Random forest predictions.csv',names=['index','class'])\n",
    "predictions = predictions['class']\n",
    "\n",
    "actual = pd.read_csv('Actuals.csv',names=['index','class'])\n",
    "actual = actual['class']\n",
    "\n",
    "rf_feature_importance = pd.read_csv('Random forest feature importance.csv',names=['index','importance'])\n",
    "rf_feature_importance = rf_feature_importance['importance']\n",
    "\n",
    "bag_feature_importance = pd.read_csv('Bagging feature importance.csv',names=['index','importance'])\n",
    "bag_feature_importance = bag_feature_importance['importance']\n",
    "\n",
    "feature_names = pd.read_csv('X train_processed data_GBP_feature names.csv',names=['index','feature'])\n",
    "feature_names = feature_names['feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace class names with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace class names with labels\n",
    "actual_labels = list(set(actual))\n",
    "\n",
    "replacement_dict = {j:'Label '+str(i+1) for i,j in enumerate(actual_labels)}\n",
    "\n",
    "for key,value in replacement_dict.items():\n",
    "    predictions = [text.replace(key,value) for text in predictions]\n",
    "    actual = [text.replace(key,value) for text in actual]\n",
    "    \n",
    "con_mat_actual_labels = list(set(actual))\n",
    "con_mat_pred_labels = ['Predicted '+i for i in con_mat_actual_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy score: ',accuracy_score(actual,predictions))\n",
    "\n",
    "# Output:\n",
    "# Accuracy score:  0.9438747093840218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix\n",
    "conmat = np.array(confusion_matrix(actual, predictions, labels=con_mat_actual_labels))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=con_mat_actual_labels,\n",
    "                         columns=con_mat_pred_labels)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Predicted Label 14</th>\n",
       "      <th>Predicted Label 13</th>\n",
       "      <th>Predicted Label 5</th>\n",
       "      <th>Predicted Label 6</th>\n",
       "      <th>Predicted Label 10</th>\n",
       "      <th>Predicted Label 4</th>\n",
       "      <th>Predicted Label 8</th>\n",
       "      <th>Predicted Label 7</th>\n",
       "      <th>Predicted Label 16</th>\n",
       "      <th>Predicted Label 9</th>\n",
       "      <th>Predicted Label 1</th>\n",
       "      <th>Predicted Label 17</th>\n",
       "      <th>Predicted Label 19</th>\n",
       "      <th>Predicted Label 15</th>\n",
       "      <th>Predicted Label 11</th>\n",
       "      <th>Predicted Label 12</th>\n",
       "      <th>Predicted Label 2</th>\n",
       "      <th>Predicted Label 18</th>\n",
       "      <th>Predicted Label 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Label 14</td>\n",
       "      <td>6401</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Label 13</td>\n",
       "      <td>3</td>\n",
       "      <td>10209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Label 5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>657</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Label 6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6087</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Label 10</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Label 4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Label 8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1157</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Label 7</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5390</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Label 16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Label 9</td>\n",
       "      <td>605</td>\n",
       "      <td>1448</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>393</td>\n",
       "      <td>0</td>\n",
       "      <td>88253</td>\n",
       "      <td>417</td>\n",
       "      <td>58</td>\n",
       "      <td>115</td>\n",
       "      <td>971</td>\n",
       "      <td>1371</td>\n",
       "      <td>83</td>\n",
       "      <td>32</td>\n",
       "      <td>131</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Label 1</td>\n",
       "      <td>92</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>16345</td>\n",
       "      <td>63</td>\n",
       "      <td>17</td>\n",
       "      <td>187</td>\n",
       "      <td>76</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Label 17</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>5114</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>242</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Label 19</td>\n",
       "      <td>14</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1687</td>\n",
       "      <td>75</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Label 15</td>\n",
       "      <td>43</td>\n",
       "      <td>134</td>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>53</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>10824</td>\n",
       "      <td>123</td>\n",
       "      <td>11</td>\n",
       "      <td>194</td>\n",
       "      <td>67</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Label 11</td>\n",
       "      <td>15</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>76</td>\n",
       "      <td>10040</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Label 12</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>1110</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Label 2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>127</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7928</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Label 18</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>256</td>\n",
       "      <td>14</td>\n",
       "      <td>93</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>11197</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Label 3</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>118</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Predicted Label 14  Predicted Label 13  Predicted Label 5  \\\n",
       "0    Label 14                6401                   3                  0   \n",
       "1    Label 13                   3               10209                  0   \n",
       "2     Label 5                   2                   0                657   \n",
       "3     Label 6                   4                   0                  0   \n",
       "4    Label 10                   3                  12                  0   \n",
       "5     Label 4                   0                   0                  0   \n",
       "6     Label 8                   6                   0                  0   \n",
       "7     Label 7                   1                  52                  0   \n",
       "8    Label 16                   1                   0                  0   \n",
       "9     Label 9                 605                1448                  0   \n",
       "10    Label 1                  92                  30                  1   \n",
       "11   Label 17                  10                   4                  0   \n",
       "12   Label 19                  14                  57                  1   \n",
       "13   Label 15                  43                 134                  8   \n",
       "14   Label 11                  15                 245                  0   \n",
       "15   Label 12                  31                   4                  0   \n",
       "16    Label 2                  14                   1                  3   \n",
       "17   Label 18                  44                   3                  3   \n",
       "18    Label 3                  11                   5                  0   \n",
       "\n",
       "    Predicted Label 6  Predicted Label 10  Predicted Label 4  \\\n",
       "0                   2                   0                  0   \n",
       "1                   0                   3                  0   \n",
       "2                   0                   0                  0   \n",
       "3                6087                   0                  0   \n",
       "4                   0                 198                  0   \n",
       "5                   0                   0                  0   \n",
       "6                   0                   0                  0   \n",
       "7                   0                   1                  0   \n",
       "8                   0                   0                  0   \n",
       "9                   0                  10                  0   \n",
       "10                  4                   0                  0   \n",
       "11                  0                   0                  0   \n",
       "12                  5                   1                  0   \n",
       "13                 85                   0                  0   \n",
       "14                  0                   0                  0   \n",
       "15                  0                   2                  0   \n",
       "16                  3                   0                  0   \n",
       "17                  2                   0                  0   \n",
       "18                  2                   0                  0   \n",
       "\n",
       "    Predicted Label 8  Predicted Label 7  Predicted Label 16  \\\n",
       "0                  32                 10                   0   \n",
       "1                   1                 24                   0   \n",
       "2                   0                  0                   0   \n",
       "3                   0                  2                   0   \n",
       "4                   0                  0                   0   \n",
       "5                   0                  0                   0   \n",
       "6                1157                  0                   0   \n",
       "7                   0               5390                   0   \n",
       "8                   0                  0                 127   \n",
       "9                  17                393                   0   \n",
       "10                 19                 42                   0   \n",
       "11                  2                  7                   0   \n",
       "12                 12                 27                   0   \n",
       "13                  8                 24                   3   \n",
       "14                  0                 55                   0   \n",
       "15                  0                  2                   2   \n",
       "16                 19                  0                   3   \n",
       "17                  5                  4                   4   \n",
       "18                  7                  5                   0   \n",
       "\n",
       "    Predicted Label 9  Predicted Label 1  Predicted Label 17  \\\n",
       "0                  38                 39                  21   \n",
       "1                  87                 14                   2   \n",
       "2                   0                  1                   1   \n",
       "3                   1                  3                   1   \n",
       "4                   5                  0                   5   \n",
       "5                   1                  0                   0   \n",
       "6                   1                  7                   3   \n",
       "7                  32                  9                  12   \n",
       "8                   0                  0                   0   \n",
       "9               88253                417                  58   \n",
       "10                 81              16345                  63   \n",
       "11                  3                 19                5114   \n",
       "12                  9                 14                   4   \n",
       "13                 78                 53                  14   \n",
       "14                 84                 31                  10   \n",
       "15                  4                  8                   2   \n",
       "16                  2                 32                  19   \n",
       "17                  1                 14                 256   \n",
       "18                  5                  5                   3   \n",
       "\n",
       "    Predicted Label 19  Predicted Label 15  Predicted Label 11  \\\n",
       "0                   11                  66                  27   \n",
       "1                   19                  40                 108   \n",
       "2                    0                   1                   1   \n",
       "3                    8                  50                   4   \n",
       "4                    0                   4                  17   \n",
       "5                    0                   0                   0   \n",
       "6                    8                  14                   0   \n",
       "7                   41                  41                  58   \n",
       "8                    0                   1                   0   \n",
       "9                  115                 971                1371   \n",
       "10                  17                 187                  76   \n",
       "11                   4                  39                  12   \n",
       "12                1687                  75                  39   \n",
       "13                  40               10824                 123   \n",
       "14                  18                  76               10040   \n",
       "15                   0                  33                  20   \n",
       "16                   4                 127                   8   \n",
       "17                  14                  93                  31   \n",
       "18                   5                 118                   8   \n",
       "\n",
       "    Predicted Label 12  Predicted Label 2  Predicted Label 18  \\\n",
       "0                    8                 16                  35   \n",
       "1                    1                  0                   4   \n",
       "2                    0                  0                   4   \n",
       "3                    0                  4                   1   \n",
       "4                    1                  0                   0   \n",
       "5                    0                  0                   2   \n",
       "6                    0                 16                   5   \n",
       "7                    0                  3                   1   \n",
       "8                    0                  0                   0   \n",
       "9                   83                 32                 131   \n",
       "10                  15                 44                  42   \n",
       "11                   7                 15                 242   \n",
       "12                   2                  7                   8   \n",
       "13                  11                194                  67   \n",
       "14                   6                  8                  13   \n",
       "15                1110                  4                   1   \n",
       "16                   1               7928                  27   \n",
       "17                   4                 31               11197   \n",
       "18                   3                 11                  29   \n",
       "\n",
       "    Predicted Label 3  \n",
       "0                  19  \n",
       "1                   1  \n",
       "2                   3  \n",
       "3                   9  \n",
       "4                   4  \n",
       "5                  12  \n",
       "6                   5  \n",
       "7                   4  \n",
       "8                   0  \n",
       "9                 117  \n",
       "10                 47  \n",
       "11                 10  \n",
       "12                  3  \n",
       "13                 65  \n",
       "14                  4  \n",
       "15                 17  \n",
       "16                  7  \n",
       "17                 41  \n",
       "18               1997  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output: Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(actual,predictions))\n",
    "\n",
    "# Output:\n",
    "\n",
    "#           precision    recall  f1-score   support\n",
    "\n",
    "#      Label 1       0.96      0.96      0.96     17105\n",
    "#     Label 10       0.92      0.80      0.85       249\n",
    "#     Label 11       0.84      0.95      0.89     10605\n",
    "#     Label 12       0.89      0.90      0.89      1240\n",
    "#     Label 13       0.84      0.97      0.90     10516\n",
    "#     Label 14       0.88      0.95      0.91      6728\n",
    "#     Label 15       0.85      0.92      0.88     11774\n",
    "#     Label 16       0.91      0.98      0.95       129\n",
    "#     Label 17       0.92      0.93      0.92      5488\n",
    "#     Label 18       0.95      0.95      0.95     11747\n",
    "#     Label 19       0.85      0.86      0.85      1965\n",
    "#      Label 2       0.95      0.97      0.96      8198\n",
    "#      Label 3       0.84      0.90      0.87      2214\n",
    "#      Label 4       0.00      0.00      0.00        15\n",
    "#      Label 5       0.98      0.98      0.98       670\n",
    "#      Label 6       0.98      0.99      0.98      6174\n",
    "#      Label 7       0.90      0.95      0.93      5645\n",
    "#      Label 8       0.90      0.95      0.93      1222\n",
    "#      Label 9       1.00      0.94      0.97     94021\n",
    "\n",
    "#    micro avg       0.94      0.94      0.94    195705\n",
    "#    macro avg       0.86      0.89      0.87    195705\n",
    "# weighted avg       0.95      0.94      0.94    195705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature importance**\n",
    "\n",
    "Reviewing the features that are used by the model to determine which features are most important when predicting the outcome is useful as you can identify potentially spurious predictions or unstable models. In addition, if multiple different models prioritise the same features then it is more likely they do have predictive ability.\n",
    "\n",
    "This step reviewed the feature importance for the features used in both the Random Forest and the Bagging (decision tree) models to review if they are consistent. Again, the features have been changed to ensure sensitive data is not shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features dataframe\n",
    "features = pd.DataFrame(feature_names,columns=['feature'])\n",
    "features['rf_importance'] = rf_feature_importance\n",
    "features['bag_importance'] = bag_feature_importance\n",
    "features = features.set_index('feature')\n",
    "features.sort_values(by='rf_importance',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 20 features for random forest against same 20 for bagging\n",
    "number_of_features = 20\n",
    "rf_feature_importance = features['rf_importance'][:number_of_features]\n",
    "bag_feature_importance = features['bag_importance'][:number_of_features]\n",
    "indices = range(len(rf_feature_importance))\n",
    "names = [i.strip() for i in features[:number_of_features].index]\n",
    "width = np.min(np.diff(indices))/3.\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,12))\n",
    "ax.bar(indices,rf_feature_importance,width,color='b',label='Random Forest')\n",
    "ax.bar(indices+width,bag_feature_importance,width,color='r',label='Bagging')\n",
    "ax.set_xticks(indices+width/2)\n",
    "ax.set_xticklabels(names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output: **\n",
    "Feature importance comparison for the top 20 features (as defined by the random forest model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the model performs very well at predicting the correct classification for the invoice based on the information provided.\n",
    "\n",
    "The confusion matrix shows there are relatively few consistent errors and the feature importances show that the different models are generally in agreement about which features are important. Therefore the model is likely to be stable.\n",
    "\n",
    "This model is a good predictor of invoice classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations for real world production environments\n",
    "\n",
    "In order to use this model in a production environment a number of additional developments and adaptations need to occur, including:\n",
    "- developing a true proof of concept model that can apply to wider data than just invoices in GBP\n",
    "- adapting the code so it is more structured and process orientated, rather than investigatory (including writin classes and functions)\n",
    "- develop a standard pipeline for processing new data and running the model\n",
    "\n",
    "In addition, continuous validation could be considered, to confirm that the model is still performing well. This could be implemented in a number of ways, including:\n",
    "- continuing to compare the model predictions against manual classifications (as the manual process will still be in place for the majority of the business)\n",
    "- compare the outputs of the supervised model with outputs of a clustering model and determining the level of agreement between the two models\n",
    "\n",
    "Prior to deploying the model publicly, a true proof of concept could be developed, with a 'pickled' finalised model and, possibly, a nice user interface, potentially developed in Flask and hosted in a cloud environment (such as Azure or AWS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Appendices\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Columns in database**\n",
    "\n",
    "- Invoice Operating Unit\n",
    "    - Client internal hierarchy\n",
    "- Business Unit\n",
    "    - Client internal hierarchy\n",
    "- Service Line\n",
    "    - Client internal hierarchy\n",
    "- Region\n",
    "    - Client internal hierarchy\n",
    "- OU/Country\n",
    "    - Client internal hierarchy\n",
    "- Contract Cat\n",
    "    - Used to derive category directly\n",
    "    - Column will be excluded from dataset\n",
    "- Supplier Number\n",
    "- Supplier Name\n",
    "- Invoice Number\n",
    "    - Unique identifier - no predictive benefit so exclude\n",
    "- Invoice Type\n",
    "- Invoice Date\n",
    "- Paid Date\n",
    "- Invoice Desc\n",
    "- PO Number\n",
    "- Invoice_Amt\n",
    "- Invoice Currency\n",
    "- USD_Amt\n",
    "- V Site Country\n",
    "    - Client internal hierarchy\n",
    "- Pay Group\n",
    "    - Group of supplier?\n",
    "- Payment Status\n",
    "- Project Owning Org\n",
    "    - Group owning the project\n",
    "- Project Number\n",
    "- Country\n",
    "    - Client internal hierarchy\n",
    "- datafile\n",
    "    - Filename for source data\n",
    "- datasource\n",
    "    - Raw table for source data\n",
    "- Legacy\n",
    "    - Different parts of the group: AFW or WG\n",
    "    - Derived from datasource\n",
    "- Year\n",
    "- Contract_type\n",
    "    - Grouping of projects\n",
    "    - Determined from project name\n",
    "- Customer\n",
    "    - Determined from project name\n",
    "- Service_line\n",
    "    - Determined from project name\n",
    "- Funding_structure\n",
    "    - Determined from project name\n",
    "- InherentCategorization\n",
    "    - Used to derive category directly\n",
    "    - Column will be excluded from dataset\n",
    "- ContractDerivedCategorization\n",
    "    - Used to derive category directly\n",
    "    - Column will be excluded from dataset\n",
    "- SupplierDerivedCategorization\n",
    "    - Used to derive category directly\n",
    "    - Column will be excluded from dataset\n",
    "- InherentCategorization_Group\n",
    "    - Used to derive category directly\n",
    "    - Column will be excluded from dataset\n",
    "- ContractDerivedCategorization_Group\n",
    "    - Used to derive category directly\n",
    "    - Column will be excluded from dataset\n",
    "- SupplierDerivedCategorization_Group\n",
    "    - Used to derive category directly\n",
    "    - Column will be excluded from dataset\n",
    "- Category_Name\n",
    "    - Sub classification of target\n",
    "    - Column will be excluded from dataset\n",
    "- Category_Group\n",
    "    - Target\n",
    "- Category_DirIndir\n",
    "    - Derived from category\n",
    "    - Column will be excluded from dataset\n",
    "- Supplier_Group\n",
    "    - Derived from Supplier Name\n",
    "- Supplier_Sector\n",
    "    - Derived from Supplier Name using existing fuzzy matching and GIS dataset\n",
    "- Leakage_Identifier\n",
    "    - Identifies purchases from competitors\n",
    "- Leakage_Group\n",
    "    - Competitor group identified from Leakage_Identifier\n",
    "- Serv_Mtrl\n",
    "    - Identifies if invoice is for Servies or Materials (based on PO number)\n",
    "- Reimbursable_Flag\n",
    "- Intercompany_Flag\n",
    "    - Not interested in intercompany transactions\n",
    "    - Intercompany transactions will be excluded from dataset\n",
    "- CatOverwriteFlag\n",
    "    - Flag to show if category is overwritten\n",
    "    - Column will be excluded from dataset\n",
    "- Americas_Flag\n",
    "- OriginalCatName\n",
    "    - Original category name\n",
    "    - Column will be excluded from dataset\n",
    "- Reporting_Date\n",
    "- HistoricalCat_overwrite_Flag\n",
    "    - Flag to show if category is overwritten\n",
    "    - Column will be excluded from dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
