{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse\n",
    "from itertools import compress\n",
    "import contractions, unicodedata, re\n",
    "from nltk.stem import LancasterStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and functions to be used in preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Extract features one by one for a pipeline\n",
    "    '''\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToNumeric(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Converts features to numeric for a pipeline\n",
    "    '''\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        numeric_df = pd.DataFrame()\n",
    "        numeric_df[self.column] = pd.to_numeric(X)\n",
    "        return numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Preprocessing for a pandas series containing text including:\n",
    "    \n",
    "    1. Replacing specific characters\n",
    "    2. Expanding contractions\n",
    "    3. Removing non-ASCII characters\n",
    "    4. Convert to lowercase\n",
    "    5. Remove punctuation\n",
    "    6. Stem words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, replacement_dictionary=None, column_header=None):\n",
    "        self.replacement_dictionary = replacement_dictionary\n",
    "        self.column_header = column_header\n",
    "    \n",
    "    def _replace_characters(self, X, *args):\n",
    "        '''\n",
    "        Replaces specific characters in the columns_to_process of X based on a replacement_dictionary\n",
    "        '''\n",
    "        replaced_df = pd.DataFrame()\n",
    "        data = X\n",
    "        for key,value in self.replacement_dictionary.items():\n",
    "            data = [text.replace(key,value) for text in data]\n",
    "        replaced_df = data\n",
    "        return replaced_df    \n",
    "    \n",
    "    def _expand_contractions(self, X, *args):\n",
    "        '''\n",
    "        Replaces contractions with the expanded form of the word (e.g. can't to cannot) in the columns_to_process of X\n",
    "        '''\n",
    "        replaced_df = pd.DataFrame()\n",
    "        data = X\n",
    "        data = [contractions.fix(text) for text in data]\n",
    "        replaced_df = data\n",
    "        return replaced_df\n",
    "    \n",
    "    def _remove_non_ascii(self, X, *args):\n",
    "        '''\n",
    "        Removes non-ascii characters from the text in the columns_to_process of X\n",
    "        '''\n",
    "        replaced_df = pd.DataFrame()\n",
    "        data = X\n",
    "        non_ascii = []\n",
    "        for text in data:\n",
    "            text_non_ascii = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            non_ascii.append(text_non_ascii)\n",
    "        replaced_df = non_ascii\n",
    "        return replaced_df\n",
    "    \n",
    "    def _to_lowercase(self, X, *args):\n",
    "        '''\n",
    "        Converts all characters to lowercase in the columns_to_process of X\n",
    "        '''\n",
    "        replaced_df = pd.DataFrame()\n",
    "        data = X\n",
    "        lower_case = []\n",
    "        for text in data:\n",
    "            text_lower = text.lower()\n",
    "            lower_case.append(text_lower)\n",
    "        replaced_df = lower_case\n",
    "        return replaced_df\n",
    "\n",
    "    def _remove_punctuation(self, X, *args):\n",
    "        '''\n",
    "        Removes punctuation from the text in the columns_to_process of X\n",
    "        '''\n",
    "        replaced_df = pd.DataFrame()\n",
    "        data = X\n",
    "        no_punct = []\n",
    "        for text in data:\n",
    "            text_ex_punct = re.sub(r'[^\\w\\s]', '', text)\n",
    "            if text_ex_punct != '':\n",
    "                no_punct.append(text_ex_punct)\n",
    "        replaced_df = no_punct\n",
    "        return replaced_df\n",
    "    \n",
    "    def _stem_words(self, X, *args):\n",
    "        '''\n",
    "        Stems the text in the columns_to_process of X\n",
    "        '''\n",
    "        replaced_df = pd.DataFrame()\n",
    "        data = X\n",
    "        stemmed_data = []\n",
    "        stemmer = LancasterStemmer()\n",
    "        for text in data:\n",
    "            stemmed_text = []\n",
    "            for word in text.split(' '):\n",
    "                stemmed_text.append(stemmer.stem(word))\n",
    "            stemmed_text = ' '.join(stemmed_text)\n",
    "            stemmed_data.append(stemmed_text)\n",
    "        replaced_df = stemmed_data\n",
    "        return replaced_df\n",
    "\n",
    "    \n",
    "    def transform(self, X, *args):\n",
    "        '''\n",
    "        Combines all preprocessing steps for X\n",
    "        '''\n",
    "        print('Initialising replacing characters...')\n",
    "        text_data = self._replace_characters(X)\n",
    "        print('Completed replacing characters')\n",
    "        print('Initialising expanding contractions...')\n",
    "        text_data = self._expand_contractions(text_data)\n",
    "        print('Completed expanding contractions')\n",
    "        print('Initialising removing non-ascii characters...')\n",
    "        text_data = self._remove_non_ascii(text_data)\n",
    "        print('Completed removing non-ascii characters')\n",
    "        print('Initialising converting characters to lowercase...')\n",
    "        text_data = self._to_lowercase(text_data)\n",
    "        print('Completed converting characters to lowercase')\n",
    "        print('Initialising removal of punctuation...')\n",
    "        text_data = self._remove_punctuation(text_data)\n",
    "        print('Completed removal of punctuation')\n",
    "        print('Initialising stemming words...')\n",
    "        text_data = self._stem_words(text_data)\n",
    "        print('Completed stemming words')\n",
    "        text_data = pd.Series(data=text_data,index=X.index,name=self.column_header)\n",
    "        return text_data\n",
    "    \n",
    "    def fit(self, X, *args):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummifier(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Dummifies a pandas series\n",
    "    \n",
    "    Ensures the resulting dummified columns match the fitted data after transformation\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dummified_columns=None\n",
    "\n",
    "    def transform(self, X, *args):\n",
    "        '''\n",
    "        Dummifies X and ensures the resulting columns match self.dummified_columns (created during fitting)\n",
    "        \n",
    "        Drops any columns in dummified X that are not in self.dummified_columns\n",
    "        Adds a zero column for any columns in  self.dummified_columns that are not in dummified X\n",
    "        '''\n",
    "        # Dummify specific columns of X\n",
    "        dummified_data = pd.get_dummies(X,drop_first=False)\n",
    "        \n",
    "        # Filter out dummified columns not in self.dummified_columns\n",
    "        col_in_fit = list(compress(dummified_data.columns, dummified_data.columns.isin(self.dummified_columns)))\n",
    "        dummified_data = dummified_data[col_in_fit]\n",
    "        \n",
    "        # Add columns in self.dummified_columns that are not in dummified X\n",
    "        col_not_in_fit = list(set(self.dummified_columns)-set(dummified_data.columns))\n",
    "        for col in col_not_in_fit:\n",
    "            dummified_data[col] = 0\n",
    "\n",
    "        return dummified_data\n",
    "\n",
    "\n",
    "    def fit(self, X, *args):\n",
    "        '''\n",
    "        Creates an index of dummified columns after dummification of X\n",
    "        Stored as self.dummified_columns\n",
    "        '''\n",
    "        # Dummify specific columns of X\n",
    "        dummified_data = pd.get_dummies(X,drop_first=True)\n",
    "        \n",
    "        # Store new columns headers as self.dummified_columns\n",
    "        self.dummified_columns = dummified_data.columns\n",
    "        \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process for creating pipeline and testing the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X_train\n",
    "- Undersampling\n",
    "- PreProcessing (fit)\n",
    "- PreProcessing (transform)\n",
    "- Oversampling\n",
    "- Model (fit)\n",
    "- Pickle processing and model\n",
    "\n",
    "#### X_test\n",
    "- Unpickle preprocessing\n",
    "- Unpickle model\n",
    "- PreProcessing (transform)\n",
    "- Model (predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    '\\r':' ',\n",
    "    '\\n':' '\n",
    "}\n",
    "\n",
    "# Create pipelines for each column\n",
    "\n",
    "invoice_desc_pipe = make_pipeline(\n",
    "    FeatureExtractor('Invoice Desc'),\n",
    "    TextPreprocessor(replacement_dictionary=replace_dict,column_header='Invoice Desc'),\n",
    "    CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    ")\n",
    "\n",
    "supplier_pipe = make_pipeline(\n",
    "    FeatureExtractor('Supplier Name'),\n",
    "    CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    ")\n",
    "\n",
    "currency_pipe = make_pipeline(\n",
    "    FeatureExtractor('Invoice Currency'),\n",
    "    CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    ")\n",
    "\n",
    "project_pipe = make_pipeline(\n",
    "    FeatureExtractor('Project Owning Org'),\n",
    "    CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    ")\n",
    "\n",
    "supp_grp_pipe = make_pipeline(\n",
    "    FeatureExtractor('Supplier_Group'),\n",
    "    CountVectorizer(token_pattern='\\w+',stop_words='english',max_df=1.0,min_df=10)\n",
    ")\n",
    "\n",
    "b_u_pipe = make_pipeline(\n",
    "    FeatureExtractor('Business Unit'),\n",
    "    Dummifier()\n",
    ")\n",
    "\n",
    "datasource_pipe = make_pipeline(\n",
    "    FeatureExtractor('datasource'),\n",
    "    Dummifier()\n",
    ")\n",
    "\n",
    "legacy_pipe = make_pipeline(\n",
    "    FeatureExtractor('Legacy'),\n",
    "    Dummifier()\n",
    ")\n",
    "\n",
    "leakage_id_pipe = make_pipeline(\n",
    "    FeatureExtractor('Leakage_Identifier'),\n",
    "    Dummifier()\n",
    ")\n",
    "\n",
    "leakage_grp_pipe = make_pipeline(\n",
    "    FeatureExtractor('Leakage_Group'),\n",
    "    Dummifier()\n",
    ")\n",
    "\n",
    "americas_pipe = make_pipeline(\n",
    "    FeatureExtractor('Americas_Flag'),\n",
    "    Dummifier()\n",
    ")\n",
    "\n",
    "invoice_amt_pipe = make_pipeline(\n",
    "    FeatureExtractor('Invoice_Amt'),\n",
    "    ToNumeric('Invoice_Amt'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "usd_amt_pipe = make_pipeline(\n",
    "    FeatureExtractor('USD_Amt'),\n",
    "    ToNumeric('USD_Amt'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "year_pipe = make_pipeline(\n",
    "    FeatureExtractor('Year'),\n",
    "    ToNumeric('Year'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "\n",
    "# Union pipelines together\n",
    "\n",
    "data_processing = make_union(invoice_desc_pipe,\n",
    "                           supplier_pipe,\n",
    "                           currency_pipe,\n",
    "                           project_pipe,\n",
    "                           supp_grp_pipe,\n",
    "                           b_u_pipe,\n",
    "                           datasource_pipe,\n",
    "                           legacy_pipe,\n",
    "                           leakage_id_pipe,\n",
    "                           leakage_grp_pipe,\n",
    "                           americas_pipe,\n",
    "                           invoice_amt_pipe,\n",
    "                           usd_amt_pipe,\n",
    "                           year_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('01 English by dropping_GBP.csv',na_values='Unknown')\n",
    "\n",
    "X = data.copy()\n",
    "y = X.pop('Category_Group')\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rebalance classes for training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categories with counts < 100\n",
    "# NB: these will never be predicted by the final model\n",
    "\n",
    "data_value_counts = pd.DataFrame(y_train.value_counts(dropna=False))\n",
    "\n",
    "categories_overly_low = data_value_counts[data_value_counts['Category_Group']>100].index\n",
    "\n",
    "y_train = y_train[y_train.isin(categories_overly_low)]\n",
    "\n",
    "X_train = X_train.loc[y_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce number of records for categories with record counts > 10000\n",
    "\n",
    "categories_too_high = data_value_counts[data_value_counts['Category_Group']>10000].index\n",
    "\n",
    "y_train_too_high = y_train[y_train.isin(categories_too_high)].copy()\n",
    "\n",
    "X_train_too_high = X_train.loc[y_train_too_high.index].copy()\n",
    "\n",
    "y_train_remainder = y_train[~y_train.isin(categories_too_high)].copy()\n",
    "\n",
    "X_train_remainder = X_train.loc[y_train_remainder.index].copy()\n",
    "\n",
    "under_sampler = RandomUnderSampler(random_state=1)\n",
    "\n",
    "X_train_undersampled, y_train_undersampled = under_sampler.fit_sample(X_train_too_high,y_train_too_high)\n",
    "\n",
    "y_train_undersampled = pd.Series(y_train_undersampled)\n",
    "\n",
    "y_train = y_train_undersampled.append(y_train_remainder,ignore_index=True)\n",
    "\n",
    "X_train_undersampled = pd.DataFrame(X_train_undersampled,columns=X_train.columns)\n",
    "\n",
    "X_train = X_train_undersampled.append(X_train_remainder,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit pipeline and transform training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processing.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = data_processing.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Under sample large classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SMOTE(random_state=1,n_jobs=-1)\n",
    "X_resampled, y_resampled = sampler.fit_sample(X_train_sparse, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_resampled).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model and train on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_resampled,y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sparse = data_processing.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy of model on test data and compare to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_predictions = rf_model.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,rf_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_test = (y_test.value_counts()/len(y_test))[0]\n",
    "baseline_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and save preprocesssing pipeline and model .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data_processing pipeline and model to disk\n",
    "preprocessing_filename = 'data_processing.pkl'\n",
    "model_filename = 'model.pkl'\n",
    "pickle.dump(data_processing, open(preprocessing_filename, 'wb'))\n",
    "pickle.dump(rf_model, open(model_filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
